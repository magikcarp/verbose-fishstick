#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" Establishes architecture for UNet model. """

from time import time

import torch
import torch.nn as nn


class UNet(nn.Module):
    """ 
    UNet network for convolving image data.

    Attributes:
        down_conv (nn.ModuleList[DoubleConvolution]): series of double 
            convolutions in the image encoding arm. 
        down_sample (nn.ModuleList[DownSample]): pooling layers with kernel size
            of 2 x 2 and a stride of 2. 
        middle_conv (DoubleConvolution): the bottom convolution of the U-Net.
        up_conv (nn.ModuleList[DoubleConvolution]): series of double 
            convolutions in the image decoding arm. 
        up_sample (nn.ModuleList[UpSample]): series of deconvolutions. 
        concat (nn.ModuleList[Concat]): performs concatenations between same 
            sized layers in the U-Net architecture. 
        last_conv (DoubleConvolution): final convolution that outputs number of 
            out channels. 
        sigmoid (nn.Sigmoid): performs normalization of logits to [0, 1]. 

    The number of kernels can be customized if desired. Example network sizes:
        - UNet : [64, 128, 256, 512, 1024]
        - SmallUNet : [32, 64, 128, 256, 512]
        - MiniUNet : [16, 32, 64, 128, 256]
        - TinyUNet : [8, 16, 32, 64, 128]
        - DeepUNet : [8, 16, 32, 64, 128, 256, 512, 1024]
            - Should only be used with very large images due to down sampling
            (image demnsions are halved after each layer of double convolution)
    """
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 n_layer_kernels: list[int]=[64, 128, 256, 512, 1024]):
        """ 
        Creates a UNet model with desired number of in and out channels.

        Args:
            in_channels (int): number of channels in the original image.
            out_channels (int): number of desired output channels.
            n_layer_kernels (list[int]): the number of kernels in each layer of
                the network. It is highly recommended that values are sequential
                powers of 2. Default is [64, 128, 256, 512, 1024].
        """
        super(UNet, self).__init__()

        n_kern = n_layer_kernels.copy() # in case input shouldn't mutate
        n_kern.insert(0, in_channels)
        down_kerns = [(n_kern[i], n_kern[i+1]) for i in range(len(n_kern)-2)]
        up_kerns = [(n_kern[i], n_kern[i-1]) for i in range(len(n_kern)-1, 1, -1)]
        n_steps = len(down_kerns)

        # Encoding arm
        self.down_conv = nn.ModuleList(
            [DoubleConvolution(i, o) for i, o in down_kerns]
        )
        self.down_sample = nn.ModuleList([DownSample() for _ in range(n_steps)])

        # Bottom of the U
        self.middle_conv = DoubleConvolution(
            n_kern[len(n_kern)-2], n_kern[len(n_kern)-1]
        )

        # Decoding arm
        self.up_conv = nn.ModuleList(
            [DoubleConvolution(i, o) for i, o in up_kerns]
        )
        self.up_sample = nn.ModuleList(
            [UpSample(i, o) for i, o in up_kerns]
        )
        self.concat = nn.ModuleList(Concat() for _ in range(n_steps))

        # Output
        self.last_conv = nn.Conv2d(n_kern[1], out_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ 
        Forward pass of data x through UNet architecture.

        The tensor x is passes through each segment of the encoding and
        decoding portion of the network before a final 1x1 convolutional layer
        and sigmoidal activation returning a shape of [1 x 512 x 512]. A mask
        can be generated by thresholding the output at a desired value.

        Args:
            x (torch.Tensor): BATCH_SIZE x 1 x 512 x 512 grayscale image.

        Returns:
            torch.Tensor: mask of the provided image.
        """
        through_pass = []
        for i in range(len(self.down_conv)):
            x = self.down_conv[i](x)
            through_pass.append(x)
            x = self.down_sample[i](x)
        x = self.middle_conv(x)
        for i in range(len(self.up_conv)):
            x = self.up_sample[i](x)
            x = self.concat[i](x, through_pass.pop())
            x = self.up_conv[i](x)
        x = self.last_conv(x)
        x = self.sigmoid(x)
        return x
    
    def train_one_epoch(
            self,
            train_loader: torch.utils.data.DataLoader,
            optimizer: torch.optim.Optimizer,
            loss_fn: nn.Module,
            report_iter: int=None,
    ) -> float:
        """
        Train UNet for one epoch with a given training dataset. 

        Args: 
            train_loader (torch.utils.data.DataLoader): training data loader.
            optimizer (torch.optim.Optimizer): optimizing function.
            loss_fn (nn.Module): function to calculate loss.
            report_iter (int, optional): Prints loss every nth batch. 

        Returns:
            float: average loss across iterations in epoch. 
        """
        self.train()
        running_loss = 0.0

        start = time()
        for i, batch in enumerate(train_loader):
            images = batch["image"]
            masks = batch["mask"]

            optimizer.zero_grad()
            y_pred = self(images)
            loss = loss_fn(y_pred, masks)
            running_loss += loss.item()
            loss.backward()
            optimizer.step()

            if report_iter:
                if i % report_iter == report_iter - 1:
                    end = time()
                    print(f"  batch {i+1} loss: {loss.item():.6f} [{end-start:.4f} seconds]")
                    start = time()

        avg_loss = running_loss / len(train_loader)
        return avg_loss
    
    def train_model(
            self,
            train_loader: torch.utils.data.DataLoader,
            n_epochs: int,
            optimizer: torch.optim.Optimizer,
            loss_fn: nn.Module,
            val_loader: torch.utils.data.DataLoader=None,
            report_iter: int=5,
        ) -> dict[str, list[float]]:
        """ 
        Train the UNet model using the training data. 

        Args:
            train_loader (torch.utils.data.DataLoader): training data loader.
            n_epochs (int): the number of training rounds through the dataset.
            optimizer (torch.optim.Optimizer): optimizing function.
            loss_fn (nn.Module): function to calculate loss.
            val_loader (torch.utils.data.DataLoader): valditation data loader.
            report_iter (int): Prints loss every nth batch. Default is 25.

        Returns:
            dict[str, list[float]]
        """
        results = {"train_loss": [None] * n_epochs}
        if val_loader:
            results["val_loss"] = [None] * n_epochs

        start = time()
        for e in range(n_epochs):
            print(f"EPOCH {e+1}:")
            e_loss = self.train_one_epoch(
                train_loader, 
                optimizer, 
                loss_fn,
                report_iter,
            )
            results["train_loss"][e] = e_loss

            if val_loader:
                self.eval()
                running_val_loss = 0
                with torch.no_grad():
                    for _, vbatch in enumerate(val_loader):
                        vimage = vbatch["image"]
                        vmask = vbatch["mask"]
                        vimage_pred = self(vimage)
                        val_loss = loss_fn(vimage_pred, vmask)
                        running_val_loss += val_loss.item()
                avg_val_loss = running_val_loss / len(val_loader)  
                results["val_loss"][e] = avg_val_loss
                end = time()
                print(f"  TRAIN: {e_loss} VAL: {avg_val_loss} [{end-start:.4f} seconds]")
            else:
                end = time()
                print(f"  TRAIN: {e_loss} [{end-start:.4f} seconds]")
            start = time()
        
        return results


class DoubleConvolution(nn.Module):
    """ 
    Neural network layer that performs two convolutions with a 3x3 kernel,
    each followed by ReLU activation. 
    """
    def __init__(self, in_channels: int, out_channels: int):
        super(DoubleConvolution, self).__init__()
        self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.a1 = nn.ReLU()
        self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.a2 = nn.ReLU()
        
    def forward(self, x: torch.Tensor):
        """ Forward pass through a double convolution. """
        x = self.c1(x)
        x = self.a1(x)
        x = self.c2(x)
        x = self.a2(x)
        return x


class DownSample(nn.Module):
    """ Max pool with a kernel size and stride of 2. """
    def __init__(self):
        super(DownSample, self).__init__()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x: torch.Tensor):
        """ Forward pass through 2D pooling layer. """
        return self.pool(x)


class Concat(nn.Module):
    """ 
    Concatenates two tensors of similar shape along channel dimsension.

    Concat assumes the incoming data is of the shape [B x C x W x H] where B
    is the batches, C is the number of channels, and W and H are the width and
    height of the image. 
    """
    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):
        """ Forward pass through concatenation of two tensors. """
        return torch.concat([x, contracting_x], dim=1)


class UpSample(nn.Module):
    """ Performs upsampling of data with a kernel size and stride of 2. """
    def __init__(self, in_channels: int, out_channels: int):
        super(UpSample, self).__init__()
        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)

    def forward(self, x: torch.Tensor):
        """ Forward pass through upsampling process. """
        return self.up(x)


def seed_unet_kernels(first: int, n_layers: int) -> list[int]:
    """ 
    Constructs list of number of kernels in layer of convolutions.

    The value `first` indicates the first value in the list and subsequent
    values are the first value multiplied by a power of 2. 

    Args:
        first (int): the first value in list of kernels for initial convolution
        n_layers (int): the number of desired layers in the UNet

    Returns:
        list[int] 
    """
    return [first * (2**i) for i in range(n_layers)]
